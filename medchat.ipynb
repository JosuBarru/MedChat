{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7066245,"sourceType":"datasetVersion","datasetId":4068802},{"sourceId":8323662,"sourceType":"datasetVersion","datasetId":4944431},{"sourceId":8429850,"sourceType":"datasetVersion","datasetId":5020045},{"sourceId":8434812,"sourceType":"datasetVersion","datasetId":5023811},{"sourceId":178081017,"sourceType":"kernelVersion"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install git+https://github.com/huggingface/transformers.git -q\n!pip install langchain sentence_transformers faiss-gpu chromadb huggingface_hub langchain_community -q\n!pip install --upgrade torch datasets accelerate peft bitsandbytes trl -q","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:00:35.470301Z","iopub.execute_input":"2024-05-15T18:00:35.470609Z","iopub.status.idle":"2024-05-15T18:04:29.444858Z","shell.execute_reply.started":"2024-05-15T18:00:35.470584Z","shell.execute_reply":"2024-05-15T18:04:29.443733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Importamos librerías","metadata":{}},{"cell_type":"code","source":"import os, json,re\nimport pandas as pd\nfrom langchain_community.document_loaders import TextLoader\n#from InstructorEmbedding import INSTRUCTOR\n#from langchain.embeddings import HuggingFaceInstructEmbeddings\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n#import pickle\nfrom langchain.docstore.document import Document\n#import faiss\nfrom langchain.vectorstores import FAISS\n#from langchain import FAISS\nfrom langchain.vectorstores import Chroma\nfrom langchain.chains import RetrievalQA\n#from tqdm.autonotebook import trange\nfrom langchain.llms import HuggingFacePipeline\nfrom langchain.memory import ConversationBufferMemory\nimport transformers, accelerate\nimport torch\nfrom transformers import AutoTokenizer,BitsAndBytesConfig,AutoModelForCausalLM,GenerationConfig,pipeline\nfrom typing import List\nfrom langchain.embeddings.base import Embeddings\nfrom sentence_transformers import SentenceTransformer\nfrom langchain_community.embeddings import JinaEmbeddings\nfrom langchain.chains import VectorDBQA, RetrievalQA,ConversationalRetrievalChain\nfrom langchain_core.prompts import PromptTemplate\nfrom langchain_core.runnables import RunnableParallel, RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain.output_parsers import StructuredOutputParser, ResponseSchema\nfrom langchain import LLMChain\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\n#from llama_index.embeddings.huggingface import HuggingFaceEmbedding\naccess_token_read = UserSecretsClient().get_secret(\"HuggingFace\")\nlogin(token = access_token_read)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:04:29.447221Z","iopub.execute_input":"2024-05-15T18:04:29.447608Z","iopub.status.idle":"2024-05-15T18:04:46.709116Z","shell.execute_reply.started":"2024-05-15T18:04:29.447570Z","shell.execute_reply":"2024-05-15T18:04:46.708119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocesado \n\nEl preprocesado de datos para el retriever lo hago de dos maneras diferentes:\n- En la primera divido los abstracts y saco chunks de 1000\n- En la segunda cojo todo el abstract entero ","metadata":{}},{"cell_type":"markdown","source":"Cargamos los datos del fichero en formato txt","metadata":{}},{"cell_type":"code","source":"#loader = TextLoader(\"/kaggle/input/100000abstracts/1000000.txt\")\n#pages = loader.load()","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:04:46.710350Z","iopub.execute_input":"2024-05-15T18:04:46.710887Z","iopub.status.idle":"2024-05-15T18:04:46.715956Z","shell.execute_reply.started":"2024-05-15T18:04:46.710860Z","shell.execute_reply":"2024-05-15T18:04:46.714964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Prueba para dividir teniendo en cuenta número de tokens y no de palabras.","metadata":{}},{"cell_type":"code","source":"#tokenizer = AutoTokenizer.from_pretrained('hkunlp/instructor-large') # initialize the INSTRUCTOR tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:04:46.718657Z","iopub.execute_input":"2024-05-15T18:04:46.719086Z","iopub.status.idle":"2024-05-15T18:04:46.741196Z","shell.execute_reply.started":"2024-05-15T18:04:46.719057Z","shell.execute_reply":"2024-05-15T18:04:46.740180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(tokenizer, chunk_size=512, chunk_overlap=100, separators=[\"\\n\"])\n#chunks = splitter.split_documents(pages)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:04:46.742289Z","iopub.execute_input":"2024-05-15T18:04:46.742539Z","iopub.status.idle":"2024-05-15T18:04:46.751923Z","shell.execute_reply.started":"2024-05-15T18:04:46.742518Z","shell.execute_reply":"2024-05-15T18:04:46.751083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"--------------------------------------","metadata":{}},{"cell_type":"markdown","source":"Dividimos por abstract y descartamos los títulos","metadata":{}},{"cell_type":"code","source":"#def split_text_into_chunks(text):\n    # Split the text into paragraphs\n#    paragraphs = text.split('\\n')[:-1] #ultimo \\n\n#    chunk_pairs = [Document(page_content=paragraphs[i + 1], metadata={\"source\": paragraphs[i]}) for i in range(0, len(paragraphs), 2)]\n#    return chunk_pairs","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:04:46.753595Z","iopub.execute_input":"2024-05-15T18:04:46.754115Z","iopub.status.idle":"2024-05-15T18:04:46.761464Z","shell.execute_reply.started":"2024-05-15T18:04:46.754083Z","shell.execute_reply":"2024-05-15T18:04:46.760668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#chunks = split_text_into_chunks(pages[0].page_content)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:04:46.762653Z","iopub.execute_input":"2024-05-15T18:04:46.762988Z","iopub.status.idle":"2024-05-15T18:04:46.772370Z","shell.execute_reply.started":"2024-05-15T18:04:46.762959Z","shell.execute_reply":"2024-05-15T18:04:46.771484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(temas[1])","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:04:46.773594Z","iopub.execute_input":"2024-05-15T18:04:46.773838Z","iopub.status.idle":"2024-05-15T18:04:46.781724Z","shell.execute_reply.started":"2024-05-15T18:04:46.773816Z","shell.execute_reply":"2024-05-15T18:04:46.781034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Subdividir los abstracts","metadata":{}},{"cell_type":"code","source":"#splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50, separators=[\".\"])\n#chunks = splitter.split_documents(temas)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:04:46.782770Z","iopub.execute_input":"2024-05-15T18:04:46.783030Z","iopub.status.idle":"2024-05-15T18:04:46.794308Z","shell.execute_reply.started":"2024-05-15T18:04:46.783009Z","shell.execute_reply":"2024-05-15T18:04:46.793450Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#chunks[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:04:46.797864Z","iopub.execute_input":"2024-05-15T18:04:46.798172Z","iopub.status.idle":"2024-05-15T18:04:46.805249Z","shell.execute_reply.started":"2024-05-15T18:04:46.798144Z","shell.execute_reply":"2024-05-15T18:04:46.804347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#chunks[1]","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:04:46.806253Z","iopub.execute_input":"2024-05-15T18:04:46.806545Z","iopub.status.idle":"2024-05-15T18:04:46.815249Z","shell.execute_reply.started":"2024-05-15T18:04:46.806512Z","shell.execute_reply":"2024-05-15T18:04:46.814480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Modelo para crear los embeddings para el vectorStore, también es necesario para el retriever, para poder crear los embeddings de la query","metadata":{}},{"cell_type":"code","source":"class SentenceTransformerEmbeddings(Embeddings):\n    def __init__(self, model_name: str):\n        self.model = SentenceTransformer(model_name, trust_remote_code=True)\n        self.model.max_seq_length=8000\n\n    def embed_documents(self, documents: List[str]) -> List[List[float]]:\n        return self.model.encode(documents, batch_size=20, show_progress_bar=True)\n\n    def embed_query(self, query: str) -> List[float]:\n        return self.model.encode([query])[0]","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:04:46.816277Z","iopub.execute_input":"2024-05-15T18:04:46.816583Z","iopub.status.idle":"2024-05-15T18:04:46.825208Z","shell.execute_reply.started":"2024-05-15T18:04:46.816554Z","shell.execute_reply":"2024-05-15T18:04:46.824452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Creamos la vectorStore","metadata":{}},{"cell_type":"code","source":"#embedding = SentenceTransformerEmbeddings(\"jinaai/jina-embeddings-v2-base-es\")\n\n#db = FAISS.from_documents(chunks, embedding)\n#db.save_local(\"/kaggle/working/faissNoChunks\")","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:04:46.826135Z","iopub.execute_input":"2024-05-15T18:04:46.827716Z","iopub.status.idle":"2024-05-15T18:04:46.836255Z","shell.execute_reply.started":"2024-05-15T18:04:46.827693Z","shell.execute_reply":"2024-05-15T18:04:46.835380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Cargamos la vectorStore ya creada","metadata":{}},{"cell_type":"code","source":"embedding = SentenceTransformerEmbeddings(\"jinaai/jina-embeddings-v2-base-es\")\ndb = FAISS.load_local(\"/kaggle/input/embeddings/faissNoChunks\", embedding,allow_dangerous_deserialization=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:04:46.837313Z","iopub.execute_input":"2024-05-15T18:04:46.837603Z","iopub.status.idle":"2024-05-15T18:05:31.994528Z","shell.execute_reply.started":"2024-05-15T18:04:46.837562Z","shell.execute_reply":"2024-05-15T18:05:31.993690Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"db.similarity_search_with_score(\"Practico deporte habitualmente, hace poco un dedo me crujió y ahora me duele\",k=3, fetch_k=5)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:05:31.995783Z","iopub.execute_input":"2024-05-15T18:05:31.996177Z","iopub.status.idle":"2024-05-15T18:05:33.186306Z","shell.execute_reply.started":"2024-05-15T18:05:31.996141Z","shell.execute_reply":"2024-05-15T18:05:33.185378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modelo de Lenguaje","metadata":{}},{"cell_type":"code","source":"MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\"\n\n# Quantization is a technique used to reduce the memory and computation requirements \n# of deep learning models, typically by using fewer bits, 4 bits\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_use_double_quant=True,\n)\n\n# Initialization of a tokenizer for the Mistral-7b model, \n# necessary to preprocess text data for input\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Initialization of the pre-trained language Mistral-7b\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME, torch_dtype=torch.float16,\n    trust_remote_code=True,\n    device_map=\"auto\",\n    quantization_config=quantization_config\n)\n\n# Configuration of some generation-related settings\ngeneration_config = GenerationConfig.from_pretrained(MODEL_NAME)\ngeneration_config.max_new_tokens = 100 # maximum number of new tokens that can be generated by the model\ngeneration_config.repetition_penalty = 1.15 # the degree to which the model should avoid repeating tokens in the generated text\n\n# A pipeline is an object that works as an API for calling the model\n# The pipeline is made of (1) the tokenizer instance, the model instance, and\n# some post-procesing settings. Here, it's configured to return full-text outputs\npipe = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    return_full_text=False,\n    generation_config=generation_config,\n)\n\nllm = HuggingFacePipeline(pipeline=pipe)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pruebas\nProbamos el chatbot por primera vez","metadata":{}},{"cell_type":"code","source":"memory = ConversationBufferMemory(return_messages=True,memory_key='chat_history')\nretriever = db.as_retriever(search_kwargs={\"k\": 3})\nqa_chain = ConversationalRetrievalChain.from_llm(llm=LLM,retriever = retriever,memory=memory)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:07:44.933319Z","iopub.execute_input":"2024-05-15T18:07:44.933690Z","iopub.status.idle":"2024-05-15T18:07:44.938495Z","shell.execute_reply.started":"2024-05-15T18:07:44.933655Z","shell.execute_reply":"2024-05-15T18:07:44.937581Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"Me duele el hombro derecho, el dolor aumenta por la noche, no me he dado ningun golpe, qué puede ser?\"\nresponse = qa_chain({'question':query,'chat_history':[]})\nresponse['answer']","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:07:44.939574Z","iopub.execute_input":"2024-05-15T18:07:44.939842Z","iopub.status.idle":"2024-05-15T18:07:44.955765Z","shell.execute_reply.started":"2024-05-15T18:07:44.939819Z","shell.execute_reply":"2024-05-15T18:07:44.954946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"Qué me aconsejas para recuperarme lo antes posible?\"\nresponse = qa_chain({'question':query,'chat_history':[]})\nresponse['answer']","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:07:44.956837Z","iopub.execute_input":"2024-05-15T18:07:44.957103Z","iopub.status.idle":"2024-05-15T18:07:44.969908Z","shell.execute_reply.started":"2024-05-15T18:07:44.957081Z","shell.execute_reply":"2024-05-15T18:07:44.969129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# MC QA","metadata":{}},{"cell_type":"markdown","source":"Vamos a evaluar el chatbot en un dataset multiple choice","metadata":{}},{"cell_type":"markdown","source":"#### Preprocesar dataset\nElimino instancias que sean multiple en vez de single choice ","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/medmcqa-medical-mcq-dataset/validation.csv\")\ndf","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(df[df['choice_type'] == 'multi'].index)\ndf = df.drop(df[df['cop'] == 'nan'].index)\nlen(df)","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:09:04.948221Z","iopub.execute_input":"2024-05-15T18:09:04.948931Z","iopub.status.idle":"2024-05-15T18:09:04.961063Z","shell.execute_reply.started":"2024-05-15T18:09:04.948874Z","shell.execute_reply":"2024-05-15T18:09:04.960177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=df.reset_index(drop=True)\ndf=df.fillna('None')\nlen(df)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:09:05.317839Z","iopub.execute_input":"2024-05-15T18:09:05.318744Z","iopub.status.idle":"2024-05-15T18:09:05.332019Z","shell.execute_reply.started":"2024-05-15T18:09:05.318707Z","shell.execute_reply":"2024-05-15T18:09:05.330958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Estructuro y creo un nuevo dataFrame con la información que nos interesa","metadata":{}},{"cell_type":"code","source":"myDict={0:'a', 1:'b', 2:'c', 3:'d'}\nfor i in range(len(df)):\n    df.loc[i,'question']= df.loc[i,'question'] + \"\\n\\n### Select an answer between:\\n\\n\" +\"a: \" + str(df.iloc[i,2]) + \"\\nb: \" + str(df.iloc[i,3]) + \"\\nc: \" + str(df.iloc[i,4]) + \"\\nd: \" + str(df.iloc[i,5])\n    #print(myDict[df.iloc[i,2]])\n    df.loc[i, 'correctOption']= myDict[df.iloc[i,6]]\ndf2=df[['question', 'correctOption']]\ndisplay(df2)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-15T18:09:06.241505Z","iopub.execute_input":"2024-05-15T18:09:06.242097Z","iopub.status.idle":"2024-05-15T18:09:08.096750Z","shell.execute_reply.started":"2024-05-15T18:09:06.242061Z","shell.execute_reply":"2024-05-15T18:09:08.095864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#df_partition1 = df2.loc[250:500]","metadata":{"execution":{"iopub.status.busy":"2024-05-15T20:41:26.767164Z","iopub.execute_input":"2024-05-15T20:41:26.767566Z","iopub.status.idle":"2024-05-15T20:41:26.772711Z","shell.execute_reply.started":"2024-05-15T20:41:26.767536Z","shell.execute_reply":"2024-05-15T20:41:26.771618Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inferencia RAG","metadata":{}},{"cell_type":"code","source":"retriever = db.as_retriever(search_kwargs={\"k\": 4})\n\n\nresponse_schemas = [\n    ResponseSchema(name=\"answer\", description=\"selected choice from a, b, c or d\"),\n]\noutput_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n\nformat_instructions = output_parser.get_format_instructions()\n\n\nprompt = PromptTemplate(\n    template = \"\"\"\n### [INST] Instruction: Answer with one of the following options, return only the selected option. Here is context to help:\n\n{context}\n\n#\\n{format_instructions}\\n\n### QUESTION:\n#{question} [/INST]\"\"\", \ninput_variables=[\"context\", \"question\"],\npartial_variables={\"format_instructions\": format_instructions},\n    \n)\n\n #Create llm chain \nllm_chain = LLMChain(llm=llm, prompt=prompt)\n\nrag_chain = (RunnableParallel( \n {\"context\": retriever, \"question\": RunnablePassthrough()})\n    | llm_chain\n    #| output_parser\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = rag_chain.batch(df2.loc[:,'question'].tolist())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = open(\"/kaggle/working/restot.txt\", \"a\")\n\nfor i in res:\n    f.write(i['text'])\n\nf.close()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Procesar respuestas","metadata":{}},{"cell_type":"code","source":"pattern = r'\"answer\":\\s*\"([a-zA-Z])\"'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count=0\nfor pred,real in zip(res, df2.loc[:, \"correctOption\"].tolist()):\n    pred = re.search(pattern, str(pred['text']))\n    if pred is not None:\n        pred = pred.group(1)\n        if pred==real:\n            count+=1\nacc = count/len(res)\nprint(acc)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = open(\"/kaggle/working/acctot.txt\", \"a\")\n\nf.write(acc)\n\nf.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Inferencia sin RAG","metadata":{}},{"cell_type":"code","source":"retriever = db.as_retriever(search_kwargs={\"k\": 4})\n\n\nresponse_schemas = [\n    ResponseSchema(name=\"answer\", description=\"selected choice from a, b, c or d\"),\n]\noutput_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n\nformat_instructions = output_parser.get_format_instructions()\n\n\nprompt = PromptTemplate(\n    template = \"\"\"\n### [INST] Instruction: Answer with one of the following options, return only the selected option.\n\\n{format_instructions}\\n\n### QUESTION:\n{question} [/INST]\"\"\", \ninput_variables=[\"question\"],\npartial_variables={\"format_instructions\": format_instructions},\n    \n)\n\n# Create llm chain \nllm_chain = LLMChain(llm=llm, prompt=prompt)\n\nonly_llm_chain = (RunnableParallel( \n {\"question\": RunnablePassthrough()})\n    | llm_chain\n    #| output_parser\n)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"res = only_llm_chain.batch(df2.loc[:,'question'].tolist())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = open(\"/kaggle/working/restotNoRAG.txt\", \"a\")\n\nfor i in res:\n    f.write(i['text'])\n\nf.close()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pattern = r'\"answer\":\\s*\"([a-zA-Z])\"'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count=0\nfor pred,real in zip(res, df2.loc[:, \"correctOption\"].tolist()):\n    pred = re.search(pattern, str(pred['text']))\n    if pred is not None:\n        pred = pred.group(1)\n        if pred==real:\n            count+=1\nacc = count/len(res)\nprint(acc)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = open(\"/kaggle/working/acctotNoRAG.txt\", \"a\")\n\nf.write(acc)\n\nf.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# UI","metadata":{}},{"cell_type":"code","source":"retriever = db.as_retriever(search_kwargs={\"k\": 4})\nqa = ConversationalRetrievalChain.from_llm(\n        llm=LLM, \n        chain_type=\"stuff\", \n        retriever=retriever, \n        return_source_documents=True,\n        return_generated_question=True,\n    )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query = \"Me duele el hombro derecho, el dolor aumenta por la noche, no me he dado ningun golpe, pero practico deporte, qué puede ser?\"\nresponse = qa({'question':query,'chat_history':[]})\nresponse['answer']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import panel as pn\nimport param\n\nclass cbfs(param.Parameterized):\n    chat_history = param.List([])\n    answer = param.String(\"\")\n    db_query  = param.String(\"\")\n    db_response = param.List([])\n    \n    def __init__(self,  **params):\n        super(cbfs, self).__init__( **params)\n        self.panels = []\n        self.qa = qa\n    \n\n    def convchain(self, query):\n        if not query:\n            return pn.WidgetBox(pn.Row('User:', pn.pane.Markdown(\"\", width=600)), scroll=True)\n        result = self.qa({\"question\": query, \"chat_history\": self.chat_history})\n        self.chat_history.extend([(query, result[\"answer\"])])\n        self.db_query = result[\"generated_question\"]\n        self.db_response = result[\"source_documents\"]\n        self.answer = result['answer'] \n        self.panels.extend([\n            pn.Row('User:', pn.pane.Markdown(query, width=600)),\n            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=600, styles={'background-color': '#F6F6F6'}))\n        ])\n        inp.value = ''  #clears loading indicator when cleared\n        return pn.WidgetBox(*self.panels,scroll=True)\n\n    @param.depends('db_query ', )\n    def get_lquest(self):\n        if not self.db_query :\n            return pn.Column(\n                pn.Row(pn.pane.Markdown(f\"Last question to DB:\", styles={'background-color': '#F6F6F6'})),\n                pn.Row(pn.pane.Str(\"no DB accesses so far\"))\n            )\n        return pn.Column(\n            pn.Row(pn.pane.Markdown(f\"DB query:\", styles={'background-color': '#F6F6F6'})),\n            pn.pane.Str(self.db_query )\n        )\n\n    @param.depends('db_response', )\n    def get_sources(self):\n        if not self.db_response:\n            return \n        rlist=[pn.Row(pn.pane.Markdown(f\"Result of DB lookup:\", styles={'background-color': '#F6F6F6'}))]\n        for doc in self.db_response:\n            rlist.append(pn.Row(pn.pane.Str(doc)))\n        return pn.WidgetBox(*rlist, width=600, scroll=True)\n\n    @param.depends('convchain', 'clr_history') \n    def get_chats(self):\n        if not self.chat_history:\n            return pn.WidgetBox(pn.Row(pn.pane.Str(\"No History Yet\")), width=600, scroll=True)\n        rlist=[pn.Row(pn.pane.Markdown(f\"Current Chat History variable\", styles={'background-color': '#F6F6F6'}))]\n        for exchange in self.chat_history:\n            rlist.append(pn.Row(pn.pane.Str(exchange)))\n        return pn.WidgetBox(*rlist, width=600, scroll=True)\n\n    def clr_history(self,count=0):\n        self.chat_history = []\n        return \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pn.extension()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cb = cbfs()\n\n#file_input = pn.widgets.FileInput(accept='.pdf')\n#button_load = pn.widgets.Button(name=\"Load DB\", button_type='primary')\nbutton_clearhistory = pn.widgets.Button(name=\"Clear History\", button_type='warning')\nbutton_clearhistory.on_click(cb.clr_history)\ninp = pn.widgets.TextInput( placeholder='Enter text here…')\n\nconversation = pn.bind(cb.convchain, inp) \n\njpg_pane = pn.pane.Image( './img/convchain.jpg')\n\ntab1 = pn.Column(\n    pn.Row(inp),\n    pn.layout.Divider(),\n    pn.panel(conversation,  loading_indicator=True, height=300),\n    pn.layout.Divider(),\n)\ntab2= pn.Column(\n    pn.panel(cb.get_lquest),\n    pn.layout.Divider(),\n    pn.panel(cb.get_sources ),\n)\ntab3= pn.Column(\n    pn.panel(cb.get_chats),\n    pn.layout.Divider(),\n)\ntab4=pn.Column(\n    pn.Row( button_clearhistory, pn.pane.Markdown(\"Clears chat history. Can use to start a new topic\" )),\n    pn.layout.Divider(),\n    pn.Row(jpg_pane.clone(width=400))\n)\ndashboard = pn.Column(\n    pn.Row(pn.pane.Markdown('# ChatWithYourData_Bot')),\n    pn.Tabs(('Conversation', tab1), ('Database', tab2), ('Chat History', tab3),('Configure', tab4))\n)\ndashboard","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dashboard","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}